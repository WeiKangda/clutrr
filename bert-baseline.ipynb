{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4967acf4717b4a29\n",
      "Reusing dataset csv (C:\\Users\\hanso\\.cache\\huggingface\\datasets\\csv\\default-4967acf4717b4a29\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-4967acf4717b4a29\n",
      "Reusing dataset csv (C:\\Users\\hanso\\.cache\\huggingface\\datasets\\csv\\default-4967acf4717b4a29\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-4967acf4717b4a29\n",
      "Reusing dataset csv (C:\\Users\\hanso\\.cache\\huggingface\\datasets\\csv\\default-4967acf4717b4a29\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "train_ds = load_dataset('csv', data_files = r'C:\\Users\\hanso\\Desktop\\CLUTRR_Data\\data_emnlp_final\\data_06b8f2a1\\2.2,2.3_train.csv', split = 'train[:70%]')\n",
    "validation_ds = load_dataset('csv', data_files = r'C:\\Users\\hanso\\Desktop\\CLUTRR_Data\\data_emnlp_final\\data_06b8f2a1\\2.2,2.3_train.csv', split = 'train[70%:85%]')\n",
    "test_ds = load_dataset('csv', data_files = r'C:\\Users\\hanso\\Desktop\\CLUTRR_Data\\data_emnlp_final\\data_06b8f2a1\\2.2,2.3_train.csv', split = 'train[85%:100%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'id', 'story', 'query', 'text_query', 'target', 'text_target', 'clean_story', 'proof_state', 'f_comb', 'task_name', 'story_edges', 'edge_types', 'query_edge', 'genders', 'syn_story', 'node_mapping', 'task_split'],\n",
      "    num_rows: 7108\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'id', 'story', 'query', 'text_query', 'target', 'text_target', 'clean_story', 'proof_state', 'f_comb', 'task_name', 'story_edges', 'edge_types', 'query_edge', 'genders', 'syn_story', 'node_mapping', 'task_split'],\n",
      "    num_rows: 1523\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Unnamed: 0', 'id', 'story', 'query', 'text_query', 'target', 'text_target', 'clean_story', 'proof_state', 'f_comb', 'task_name', 'story_edges', 'edge_types', 'query_edge', 'genders', 'syn_story', 'node_mapping', 'task_split'],\n",
      "    num_rows: 1523\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>story</th>\n",
       "      <th>query</th>\n",
       "      <th>text_query</th>\n",
       "      <th>target</th>\n",
       "      <th>text_target</th>\n",
       "      <th>clean_story</th>\n",
       "      <th>proof_state</th>\n",
       "      <th>f_comb</th>\n",
       "      <th>task_name</th>\n",
       "      <th>story_edges</th>\n",
       "      <th>edge_types</th>\n",
       "      <th>query_edge</th>\n",
       "      <th>genders</th>\n",
       "      <th>syn_story</th>\n",
       "      <th>node_mapping</th>\n",
       "      <th>task_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3974</td>\n",
       "      <td>815461c7-f916-4bdf-9d73-0a80a44d7eb4</td>\n",
       "      <td>[Serena] is a woman with a sister named [Marlene]. [Robin] took her daughter, [Serena], to lunch. [Karen] took her granddaughter [Serena] shopping. [Karen] was so proud of her daughter [Robin] for getting straight A's this semester.</td>\n",
       "      <td>('Marlene', 'Karen')</td>\n",
       "      <td>None</td>\n",
       "      <td>grandmother</td>\n",
       "      <td>['[Marlene] loved played dolls with her grandmother, [Karen].']</td>\n",
       "      <td>[Serena] is a woman with a sister named [Marlene]. [Karen] took her granddaughter [Serena] shopping.</td>\n",
       "      <td>[{('Marlene', 'grandmother', 'Karen'): [('Marlene', 'sister', 'Serena'), ('Serena', 'grandmother', 'Karen')]}]</td>\n",
       "      <td>sister-grandmother</td>\n",
       "      <td>task_2.2</td>\n",
       "      <td>[(0, 1), (1, 2), (1, 3), (3, 2)]</td>\n",
       "      <td>['sister', 'grandmother']</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>Marlene:female,Serena:female,Karen:female,Robin:female</td>\n",
       "      <td>None</td>\n",
       "      <td>{8: 0, 10: 1, 0: 2, 2: 3}</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=1):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "print(train_ds)\n",
    "print(validation_ds)\n",
    "print(test_ds)\n",
    "\n",
    "show_random_elements(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and manipulation codes\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import itertools as it\n",
    "from addict import Dict\n",
    "from codes.net.batch import Batch\n",
    "from codes.utils.config import get_config\n",
    "import os\n",
    "import json\n",
    "from ast import literal_eval as make_tuple\n",
    "from torch_geometric.data import Data as GeometricData\n",
    "from torch_geometric.data import Batch as GeometricBatch\n",
    "import random\n",
    "from itertools import repeat, product\n",
    "from typing import List\n",
    "from codes.utils.bert_utils import BertLocalCache\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "base_path = os.path.dirname(os.path.realpath(__file__)).split('codes')[0]\n",
    "UNK_WORD = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "START_TOKEN = '<s>'\n",
    "END_TOKEN = '</s>'\n",
    "# bert tokens\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "SEP_TOKEN = \"[SEP]\"\n",
    "\n",
    "class DataRow():\n",
    "    \"\"\"\n",
    "    Defines a single instance of data row\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = None\n",
    "        self.story = None\n",
    "        self.story_sents = None # same story, but sentence tokenized\n",
    "        self.query = None\n",
    "        self.text_query = None\n",
    "        self.target = None\n",
    "        self.text_target = None\n",
    "        self.story_graph = None\n",
    "        # new variables to only contain the clean graph for Exp 3\n",
    "        self.story_edges = None\n",
    "        self.edge_types = None\n",
    "        self.query_edge = None\n",
    "        # processed attributes\n",
    "        self.pattrs = []\n",
    "\n",
    "\n",
    "class DataUtility():\n",
    "    \"\"\"\n",
    "    Data preparation and utility class\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 num_workers=4,\n",
    "                 common_dict=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param main_file: file where the summarization resides\n",
    "        :param train_test_split: default 0.8\n",
    "        :param sentence_mode: if sentence_mode == True, then split story into sentence\n",
    "        :param single_abs_line: if True, then output pair is single sentences of abs\n",
    "        :param num_reads: number of reads for a sentence\n",
    "        :param dim: dimension of edges\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        # derive configurations\n",
    "        self.train_test_split = config.dataset.train_test_split\n",
    "        self.max_vocab = config.dataset.max_vocab\n",
    "        self.tokenization = config.dataset.tokenization\n",
    "        self.common_dict = config.dataset.common_dict\n",
    "        self.batch_size = config.model.batch_size\n",
    "        self.num_reads = config.model.graph.num_reads\n",
    "        self.dim = config.model.graph.edge_dim\n",
    "        self.sentence_mode = config.dataset.sentence_mode\n",
    "        self.single_abs_line = config.dataset.single_abs_line\n",
    "        self.num_entity_block = config.model.num_entity_block  # number of entity vectors we want to block off\n",
    "        self.process_bert = config.dataset.process_bert\n",
    "        if self.process_bert:\n",
    "            self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.target_word2id = {}\n",
    "        self.target_id2word = {}\n",
    "        # dict of dataRows\n",
    "        # all rows are indexed by their key `id`\n",
    "        self.dataRows = {'train':{}, 'test':{}}\n",
    "\n",
    "        self.train_indices = []\n",
    "        self.test_indices = []\n",
    "        self.val_indices = []\n",
    "        self.special_tokens = [PAD_TOKEN, UNK_WORD, START_TOKEN, END_TOKEN]\n",
    "        self.main_file = ''\n",
    "        self.common_dict = common_dict\n",
    "        self.num_workers = num_workers\n",
    "        # keep some part of the vocab fixed for the entities\n",
    "        # for that we need to first calculate the max number of unique entities *per row*\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.train_file = ''\n",
    "        self.test_file = ''\n",
    "        self.max_ents = 0\n",
    "        self.entity_ids = []\n",
    "        self.entity_map = {} # map entity for each puzzle\n",
    "        self.max_entity_id = 0\n",
    "        self.adj_graph = []\n",
    "        self.dummy_entity = '' # return this entity when UNK entity\n",
    "        self.load_dictionary = config.dataset.load_dictionary\n",
    "        self.max_sent_length = 0\n",
    "        self.unique_edge_dict = {}\n",
    "        # check_data flags\n",
    "        self.data_has_query = False\n",
    "        self.data_has_text_query = False\n",
    "        self.data_has_target = False\n",
    "        self.data_has_text_target = False\n",
    "        self.data_has_raw_graph = False\n",
    "        self.preprocessed = set() # set of puzzle ids which has been preprocessed\n",
    "        self.max_sent_length = 0\n",
    "        self.max_word_length = 0\n",
    "        self.unique_nodes = set() # nodes for the raw graph\n",
    "\n",
    "    def process_data(self, base_path, train_file, load_dictionary=True, preprocess=True):\n",
    "        \"\"\"\n",
    "        Load data and run preprocessing scripts\n",
    "        :param main_file .csv file of the data\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.train_file = train_file\n",
    "        train_data = pd.read_csv(self.train_file, comment='#')\n",
    "        train_data = self._check_data(train_data)\n",
    "        logging.info(\"Start preprocessing data\")\n",
    "        if load_dictionary:\n",
    "            dictionary_file = os.path.join(base_path, 'dict.json')\n",
    "            logging.info(\"Loading dictionary from {}\".format(dictionary_file))\n",
    "            dictionary = json.load(open(dictionary_file))\n",
    "            # fix id2word keys\n",
    "            dictionary['id2word'] = {int(k):v for k,v in dictionary['id2word'].items()}\n",
    "            dictionary['target_id2word'] = {int(k): v for k, v in dictionary['target_id2word'].items()}\n",
    "            for key, value in dictionary.items():\n",
    "                setattr(self, key, value)\n",
    "        train_data, max_ents_train, = self.process_entities(train_data)\n",
    "        if preprocess:\n",
    "            self.preprocess(train_data, mode='train')\n",
    "            self.train_data = train_data\n",
    "            self.split_indices()\n",
    "        else:\n",
    "            return train_data, max_ents_train\n",
    "\n",
    "    def process_test_data(self, base_path, test_files):\n",
    "        \"\"\"\n",
    "        Load testing data\n",
    "        :param test_files: array of file names\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.test_files = test_files #[os.path.join(base_path, t) + '_test.csv' for t in test_files]\n",
    "        test_datas = [pd.read_csv(tf, comment='#') for tf in self.test_files]\n",
    "        for test_data in test_datas:\n",
    "            self._check_data(test_data)\n",
    "        logging.info(\"Loaded test data, starting preprocessing\")\n",
    "        p_tests = []\n",
    "        for ti, test_data in enumerate(test_datas):\n",
    "            test_data, max_ents_test, = self.process_entities(test_data)\n",
    "            self.preprocess(test_data, mode='test',\n",
    "                            test_file=test_files[ti])\n",
    "            p_tests.append(test_data)\n",
    "        self.test_data = p_tests\n",
    "        logging.info(\"Done preprocessing test data\")\n",
    "\n",
    "\n",
    "    def _check_data(self, data):\n",
    "        \"\"\"\n",
    "        Check if the file has correct headers.\n",
    "        For all the subsequent experiments, make sure that the dataset generated\n",
    "        or curated has the following fields:\n",
    "        - id : unique uuid for each puzzle          : required\n",
    "        - story : input text                        : required\n",
    "        - query : query entities                    : optional\n",
    "        - text_query : the question for QA models   : optional\n",
    "        - target : classification target            : required if config.model.loss_type set to classify\n",
    "        - text_target : seq2seq target              : required if config.model.loss_type set to seq2seq\n",
    "        :param data:\n",
    "        :return: data\n",
    "        \"\"\"\n",
    "        # check for required stuff\n",
    "        assert \"id\" in list(data.columns)\n",
    "        assert \"story\" in list(data.columns)\n",
    "        if self.config.model.loss_type == 'classify':\n",
    "            assert \"target\" in list(data.columns)\n",
    "        if self.config.model.loss_type == 'seq2seq':\n",
    "            assert \"text_target\" in list(data.columns)\n",
    "        # turn on flag if present\n",
    "        if \"target\" in list(data.columns):\n",
    "            self.data_has_target = True\n",
    "        if \"text_target\" in list(data.columns):\n",
    "            self.data_has_text_target = True\n",
    "        if \"query\" in list(data.columns) and len(data['query'].value_counts()) > 0:\n",
    "            self.data_has_query = True\n",
    "        else:\n",
    "            data['query'] = ''\n",
    "        if \"text_query\" in list(data.columns) and len(data['text_query'].value_counts()) > 0:\n",
    "            self.data_has_text_query = True\n",
    "        else:\n",
    "            data['text_query'] = ''\n",
    "        if \"story_edges\" in list(data.columns) and \"edge_types\" in list(data.columns) and \"query_edge\" in list(data.columns):\n",
    "            self.data_has_raw_graph = True\n",
    "        return data\n",
    "\n",
    "    def process_entities(self, data, placeholder='[]'):\n",
    "        \"\"\"\n",
    "        extract entities and replace them with placeholders.\n",
    "        Also maintain a per-puzzle mapping of entities\n",
    "        :param placeholder: if [] then simply use regex to extract entities as they are already in\n",
    "        a placeholder. If None, then use Spacy EntityTokenizer\n",
    "        :return: max number of entities in dataset\n",
    "        \"\"\"\n",
    "        max_ents = 0\n",
    "        if placeholder == '[]':\n",
    "            for i,row in data.iterrows():\n",
    "                story = row['story']\n",
    "                ents = re.findall('\\[(.*?)\\]', story)\n",
    "                uniq_ents = set(ents)\n",
    "                uniq_ents = random.sample(list(uniq_ents), len(uniq_ents))\n",
    "                pid = row['id']\n",
    "                query = row['query'] if self.data_has_query else ''\n",
    "                query = list(make_tuple(query))\n",
    "                text_query = row['text_query'] if self.data_has_text_query else ''\n",
    "                text_target = row['text_target'] if self.data_has_text_target else ''\n",
    "                entity_map = {}\n",
    "                entity_id_block = list(range(0, len(uniq_ents)))\n",
    "                for idx, ent in enumerate(uniq_ents):\n",
    "                    entity_id = random.choice(entity_id_block)\n",
    "                    entity_id_block.remove(entity_id)\n",
    "                    if self.process_bert:\n",
    "                        # if bert, then replace the entities with pure numbers, as otherwise we would not\n",
    "                        # have an unique embedding. Also, make sure the text doesn't contain any numbers before hand\n",
    "                        # TODO: remove numbers\n",
    "                        entity_map[ent] = '{}'.format(entity_id)\n",
    "                    else:\n",
    "                        entity_map[ent] = '@ent{}'.format(entity_id)\n",
    "                    story = story.replace('[{}]'.format(ent), entity_map[ent])\n",
    "                    text_target = text_target.replace('[{}]'.format(ent), entity_map[ent])\n",
    "                    text_query = text_query.replace('[{}]'.format(ent), entity_map[ent])\n",
    "                    try:\n",
    "                        ent_index = query.index(ent)\n",
    "                        query[ent_index] = entity_map[ent]\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                data.at[i, 'story'] = story\n",
    "                data.at[i, 'text_target'] = text_target\n",
    "                data.at[i, 'text_query'] = text_query\n",
    "                data.at[i, 'query'] = tuple(query)\n",
    "                data.at[i, 'entities'] = json.dumps(list(uniq_ents))\n",
    "                self.entity_map[pid] = entity_map\n",
    "                max_ents = max(max_ents, len(uniq_ents))\n",
    "        else:\n",
    "            raise NotImplementedError(\"Not implemented, should replace with a tokenization policy\")\n",
    "        self.num_entity_block = max(max_ents, self.num_entity_block)\n",
    "        return data, max_ents\n",
    "\n",
    "    def preprocess(self, data, mode='train', single_abs_line=True, test_file=''):\n",
    "        \"\"\"\n",
    "        Usual preprocessing: tokenization, lowercase, and create word dictionaries\n",
    "        Also, split stories into sentences\n",
    "        :param single_abs_line: if True, separate the abstracts into its corresponding lines\n",
    "        and add each story-abstract pairs\n",
    "        N.B. change: dropping `common_dict=True` as I am assuming I will always use a common\n",
    "        dictionary for reasoning and QA. Separate dictionary makes sense for translation which\n",
    "        I am not working at the moment.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        words = Counter()\n",
    "        max_sent_length = 0\n",
    "        max_word_length = 0\n",
    "        if self.data_has_target:\n",
    "            # assign target ids\n",
    "            self.assign_target_id(list(data['target']))\n",
    "\n",
    "        for i,row in data.iterrows():\n",
    "            dataRow = DataRow()\n",
    "            dataRow.id = row['id']\n",
    "            story_sents = sent_tokenize(row['story'])\n",
    "            if self.process_bert:\n",
    "                story_sents = [self.bert_tokenizer.tokenize(sent) for sent in story_sents]\n",
    "            else:\n",
    "                story_sents = [self.tokenize(sent) for sent in story_sents]\n",
    "            if self.process_bert:\n",
    "                story_sents = [sent + [SEP_TOKEN] for sent in story_sents]\n",
    "                story_sents[0] = [CLS_TOKEN] + story_sents[0]\n",
    "            words.update([word for sent in story_sents for word in sent])\n",
    "            dataRow.story_sents = story_sents\n",
    "            dataRow.story = [word for sent in story_sents for word in sent] # flatten\n",
    "            max_word_length = max(max_word_length, len(dataRow.story))\n",
    "            if self.data_has_text_target:\n",
    "                # preprocess text_target\n",
    "                text_target = self.tokenize(row['text_target'])\n",
    "                dataRow.text_target = text_target\n",
    "                words.update([word for word in text_target])\n",
    "            if self.data_has_text_query:\n",
    "                # preprocess text_query\n",
    "                if self.process_bert:\n",
    "                    text_query = self.bert_tokenizer.tokenize(row['text_query'])\n",
    "                else:\n",
    "                    text_query = self.tokenize(row['text_query'])\n",
    "                dataRow.text_query = text_query\n",
    "                words.update([word for word in text_query])\n",
    "            max_sl = max([len(s) for s in story_sents])\n",
    "            if max_sl > max_sent_length:\n",
    "                max_sent_length = max_sl\n",
    "            if self.data_has_query:\n",
    "                dataRow.query = row['query']\n",
    "            if self.data_has_target:\n",
    "                dataRow.target = self.target_word2id[row['target']]\n",
    "            if self.data_has_raw_graph:\n",
    "                # add the raw graph and edge ids\n",
    "                dataRow.story_edges = list(make_tuple(row['story_edges']))\n",
    "                dataRow.edge_types = make_tuple(row['edge_types'])\n",
    "                dataRow.query_edge = make_tuple(row['query_edge'])\n",
    "                unique_nodes = [n for edge in dataRow.story_edges for n in edge]\n",
    "                self.unique_nodes.update(unique_nodes)\n",
    "                for et in dataRow.edge_types:\n",
    "                    if et not in self.unique_edge_dict:\n",
    "                        self.unique_edge_dict[et] = len(self.unique_edge_dict)\n",
    "\n",
    "            if mode == 'train':\n",
    "                self.dataRows[mode][dataRow.id] = dataRow\n",
    "            else:\n",
    "                if test_file not in self.dataRows[mode]:\n",
    "                    self.dataRows[mode][test_file] = {}\n",
    "                self.dataRows[mode][test_file][dataRow.id] = dataRow\n",
    "            self.preprocessed.add(dataRow.id)\n",
    "\n",
    "        # only assign word-ids in train data\n",
    "        if mode == 'train' and not self.load_dictionary:\n",
    "            self.assign_wordids(words)\n",
    "\n",
    "        # get adj graph\n",
    "        ct = 0\n",
    "        if mode == 'train':\n",
    "            for i, row in data.iterrows():\n",
    "                dR = self.dataRows[mode][row['id']]\n",
    "                #  dR.story_graph = self.prepare_ent_graph(dR.story_sents)\n",
    "                ct += 1\n",
    "            logging.info(\"Processed {} stories in mode {}\".format(ct,\n",
    "                                                                  mode))\n",
    "            self.max_sent_length = max_sent_length\n",
    "        else:\n",
    "            for i,row in data.iterrows():\n",
    "                dR = self.dataRows[mode][test_file][row['id']]\n",
    "                # dR.story_graph = self.prepare_ent_graph(dR.story_sents)\n",
    "                ct +=1\n",
    "            logging.info(\"Processed {} stories in mode {} and file: {}\".format(\n",
    "                ct, mode, test_file))\n",
    "\n",
    "        # update the max sentence length\n",
    "        self.max_word_length = max(self.max_word_length, max_word_length)\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self, sent):\n",
    "        \"\"\"\n",
    "        tokenize sentence based on mode\n",
    "        :sent - sentence\n",
    "        :param mode: word/char\n",
    "        :return: splitted array\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        if self.tokenization == 'word':\n",
    "            words = word_tokenize(sent)\n",
    "        if self.tokenization == 'char':\n",
    "            words = sent.split('')\n",
    "        # correct for tokenizing @entity\n",
    "        corr_w = []\n",
    "        tmp_w = ''\n",
    "        for i,w in enumerate(words):\n",
    "            if w == '@':\n",
    "                tmp_w = w\n",
    "            else:\n",
    "                tmp_w += w\n",
    "                corr_w.append(tmp_w)\n",
    "                tmp_w = ''\n",
    "        return corr_w\n",
    "\n",
    "    def _insert_wordid(self, token, id):\n",
    "        if token not in self.word2id:\n",
    "            assert id not in set([v for k,v in self.word2id.items()])\n",
    "            self.word2id[token] = id\n",
    "            self.id2word[id] = token\n",
    "\n",
    "    def assign_wordids(self, words, special_tokens=None):\n",
    "        \"\"\"\n",
    "        Given a set of words, create word2id and id2word\n",
    "        :param words: set of words\n",
    "        :param special_tokens: set of special tokens to add into dictionary\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        if not special_tokens:\n",
    "            special_tokens = self.special_tokens\n",
    "        ## if max_vocab is not -1, then shrink the word size\n",
    "        if self.max_vocab >= 0:\n",
    "            words = [tup[0] for tup in words.most_common(self.max_vocab)]\n",
    "        else:\n",
    "            words = list(words.keys())\n",
    "        # add pad token\n",
    "        self._insert_wordid(PAD_TOKEN, count)\n",
    "        count +=1\n",
    "        # reserve a block for entities. Record this block for future use.\n",
    "        start_ent_num = count\n",
    "        for idx in range(self.num_entity_block):\n",
    "            self._insert_wordid('@ent{}'.format(idx), count)\n",
    "            count +=1\n",
    "        # not reserving a dummy entity now as we are reserving a whole block\n",
    "        # reserve a dummy entity\n",
    "        # self.dummy_entity = '@ent{}'.format(self.max_ents - 1)\n",
    "        # self._insert_wordid(self.dummy_entity, count)\n",
    "        # count += 1\n",
    "        end_ent_num = count\n",
    "        self.max_entity_id = end_ent_num - 1\n",
    "        self.entity_ids = list(range(start_ent_num, end_ent_num))\n",
    "        # add other special tokens\n",
    "        if special_tokens:\n",
    "            for tok in special_tokens:\n",
    "                if tok == PAD_TOKEN:\n",
    "                    continue\n",
    "                else:\n",
    "                    self._insert_wordid(tok, count)\n",
    "                    count += 1\n",
    "        # finally add the words\n",
    "        for word in words:\n",
    "            if word not in self.word2id:\n",
    "                self._insert_wordid(word, len(self.word2id))\n",
    "                #count += 1\n",
    "\n",
    "        logging.info(\"Modified dictionary. Words : {}, Entities : {}\".format(\n",
    "            len(self.word2id), len(self.entity_ids)))\n",
    "\n",
    "    def assign_target_id(self, targets):\n",
    "        \"\"\"\n",
    "        Assign IDS to targets\n",
    "        :param targets:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for target in set(targets):\n",
    "            if target not in self.target_word2id:\n",
    "                last_id = len(self.target_word2id)\n",
    "                self.target_word2id[target] = last_id\n",
    "        self.target_id2word = {v: k for k, v in self.target_word2id.items()}\n",
    "        logging.info(\"Target Entities : {}\".format(len(self.target_word2id)))\n",
    "\n",
    "    def split_indices(self):\n",
    "        \"\"\"\n",
    "        Split training file indices into training and validation\n",
    "        Now we use separate testing file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logging.info(\"splitting data ...\")\n",
    "        indices = list(self.dataRows['train'].keys())\n",
    "        mask_i = np.random.choice(indices, int(len(indices) * self.train_test_split), replace=False)\n",
    "        self.val_indices = [self.dataRows['train'][i].id for i in indices if i not in set(mask_i)]\n",
    "        self.train_indices = [self.dataRows['train'][i].id for i in indices if i in set(mask_i)]\n",
    "\n",
    "\n",
    "    def prepare_ent_graph(self, sents, max_nodes=0):\n",
    "        \"\"\"\n",
    "        Given a list of sentences, return an adjacency matrix between entities\n",
    "        Assumes entities have the format @ent{num}\n",
    "        We can use OpenIE in later editions to automatically detect entities\n",
    "        :param sents: list(list(str))\n",
    "        :param max_nodes: max number of nodes in the adjacency matrix, int\n",
    "        :return: list(list(int))\n",
    "        \"\"\"\n",
    "        if max_nodes == 0:\n",
    "            max_nodes = len(self.entity_ids)\n",
    "        adj_mat = np.zeros((max_nodes, max_nodes))\n",
    "        for sent in sents:\n",
    "            ents = list(set([w for w in sent if '@ent' in w]))\n",
    "            if len(ents) > 1:\n",
    "                for ent1, ent2 in it.combinations(ents, 2):\n",
    "                    ent1_id = self.get_entity_id(ent1) - 1\n",
    "                    ent2_id = self.get_entity_id(ent2) - 1\n",
    "                    adj_mat[ent1_id][ent2_id] = 1\n",
    "                    adj_mat[ent2_id][ent1_id] = 1\n",
    "        return adj_mat\n",
    "\n",
    "    def prepare_for_dataloader(self, dataRows:List[DataRow], bert_cache:BertLocalCache=None) -> List[DataRow]:\n",
    "        \"\"\"\n",
    "        Offload processing from dataloader get_item to here.\n",
    "        :param dataRows:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for dataRow in dataRows:\n",
    "            orig_inp = dataRow.story\n",
    "            orig_inp_sent = dataRow.story_sents\n",
    "            # This is bert_as_a_service code. Now trying hugging face code\n",
    "            # bert_inp = bert_cache.query(orig_inp_sent)\n",
    "            # here batch size is number of sentences. convert it back to one concatenation\n",
    "            # 2 x 10 x 768  -> 1 x 20 x 768\n",
    "            # bert_inp = bert_inp.view(1,-1,bert_inp.size(2))\n",
    "            bert_inp = None\n",
    "\n",
    "            # inp_row_graph = dataRow.story_graph\n",
    "            inp_row_pos = []\n",
    "\n",
    "            # for sentence tokenizations\n",
    "            sent_lengths = [len(sent) for sent in dataRow.story_sents]\n",
    "            if self.process_bert:\n",
    "                s_inp_row = [self.bert_tokenizer.convert_tokens_to_ids(sent) for sent in dataRow.story_sents]\n",
    "            else:\n",
    "                s_inp_row = [[self.get_token(word) for word in sent] for sent in dataRow.story_sents]\n",
    "            #s_inp_ents = [[id for id in sent if id in self.entity_ids] for sent in inp_row]\n",
    "            #s_inp_row_pos = [[widx + 1 for widx, word in enumerate(sent)] for sent in inp_row]\n",
    "\n",
    "            # for word tokenizations\n",
    "            # sent_lengths = [len(dataRow.story)]\n",
    "            bert_entity_dict = {}\n",
    "            if self.process_bert:\n",
    "                inp_row = [word for sent in s_inp_row for word in sent]\n",
    "                entity_ids = [str(x-1) for x in self.entity_ids] # -1 to accomodate 0\n",
    "                bert_entity_ids = self.bert_tokenizer.convert_tokens_to_ids(entity_ids)\n",
    "                for entid, b_entid in zip(entity_ids, bert_entity_ids):\n",
    "                    bert_entity_dict[b_entid] = entid\n",
    "                inp_ents = list(set(id for id in inp_row if id in bert_entity_ids))\n",
    "            else:\n",
    "                inp_row = [self.get_token(word) for word in dataRow.story]\n",
    "                inp_ents = list(set([id for id in inp_row if id in self.entity_ids]))\n",
    "\n",
    "            # bert specific variables\n",
    "            bert_input_mask = [1] * len(inp_row)\n",
    "            # for BERT, the segment ids denote each sentence.\n",
    "            bert_segment_ids = []\n",
    "            for s_id, sent in enumerate(s_inp_row):\n",
    "                bert_segment_ids.extend([0]*len(sent))\n",
    "\n",
    "\n",
    "            ## calculate one-hot mask for entities which are used in this row\n",
    "            flat_inp_ents = inp_ents\n",
    "            if self.sentence_mode:\n",
    "                flat_inp_ents = [p for x in inp_ents for p in x]\n",
    "\n",
    "            if self.process_bert:\n",
    "                inp_ent_mask = [1 if w in bert_entity_dict else 0 for w in inp_row]\n",
    "                bert_inp = [int(bert_entity_dict[w])+1 if w in bert_entity_dict else 0 for w in inp_row]\n",
    "            else:\n",
    "                inp_ent_mask = [1 if idx + 1 in flat_inp_ents else 0 for idx in range(len(self.entity_ids))]\n",
    "                bert_inp = inp_row  # dummy\n",
    "\n",
    "            # calculate for each entity pair which sentences contain them\n",
    "            # output should be a max_entity x max_entity x num_sentences --> which should be later padded\n",
    "            # if not sentence mode, then just output max_entity x max_entity x 1\n",
    "            num_sents = len(inp_row)  # 8, say\n",
    "            if self.sentence_mode:\n",
    "                assert len(inp_row) == len(inp_ents)\n",
    "                sentence_pointer = np.zeros((len(self.entity_ids), len(self.entity_ids),\n",
    "                                             num_sents))\n",
    "                for sent_idx, inp_ent in enumerate(inp_ents):\n",
    "                    if len(inp_ent) > 1:\n",
    "                        for ent1, ent2 in it.combinations(inp_ent, 2):\n",
    "                            # check if two same entities are not appearing\n",
    "                            if ent1 == ent2:\n",
    "                                raise NotImplementedError(\n",
    "                                    \"For now two same entities cannot appear in the same sentence\")\n",
    "                            assert ent1 != ent2\n",
    "                            # remember we are shifting one bit here\n",
    "                            sentence_pointer[ent1 - 1][ent2 - 1][sent_idx] = 1\n",
    "\n",
    "            else:\n",
    "                sentence_pointer = np.ones((len(self.entity_ids), len(self.entity_ids), 1))\n",
    "\n",
    "            # calculate the output\n",
    "            target = [dataRow.target]\n",
    "            if self.process_bert:\n",
    "                query = self.bert_tokenizer.convert_tokens_to_ids(list(dataRow.query))\n",
    "            else:\n",
    "                query = [self.get_token(tp) for tp in dataRow.query]  # tuple\n",
    "                # debugging\n",
    "                if self.get_token('UNKUNK') in query:\n",
    "                    print(\"shit\")\n",
    "                    raise AssertionError(\"Unknown element cannot be in the query. Check the data.\")\n",
    "            # one hot integer mask over the input text which specifies the query strings\n",
    "            query_mask = [[1 if w == ent else 0 for w in self.__flatten__(inp_row)] for ent in query]\n",
    "            # TODO: use query_text and query_text length and pass it back\n",
    "            # text_query = [self.data.get_token(tp) for tp in self.dataRows[index].text_query]\n",
    "            text_query = []\n",
    "            text_target = [START_TOKEN] + dataRow.text_target + [END_TOKEN]\n",
    "            text_target = [self.get_token(tp) for tp in text_target]\n",
    "\n",
    "            # clean graphs for GAT\n",
    "            edge_list = dataRow.story_edges  # eg, [(0, 1), (1, 2), (2, 3)]\n",
    "            edge_index = list(zip(*edge_list))  # eg, [[0, 1, 2], [1, 2, 3]]\n",
    "            edge_index = torch.LongTensor(edge_index)  # 2 x num_edges\n",
    "            edge_types = dataRow.edge_types\n",
    "            num_ue = len(self.unique_edge_dict)\n",
    "            num_e = len(edge_list)\n",
    "            edge_attr = torch.zeros(num_e, 1).long()  # [num_edges, 1]\n",
    "            # create a one-hot vector for each edge type\n",
    "            for i, e in enumerate(edge_types):\n",
    "                edge_attr[i][0] = self.unique_edge_dict[e]\n",
    "            nodes = list(set([p for x in edge_list for p in x]))\n",
    "            x = torch.arange(len(nodes)).unsqueeze(1)  # num_nodes x 1\n",
    "\n",
    "            geo_data = {'x': x, 'edge_index': edge_index, 'edge_attr': edge_attr, 'y': torch.tensor(target),\n",
    "                        'num_nodes': len(nodes)}\n",
    "            query_edge = [dataRow.query_edge]\n",
    "            num_nodes = [len(nodes)]\n",
    "            dataRow.pattrs = [inp_row, s_inp_row, inp_ents, query, text_query, query_mask, target, text_target,\n",
    "               sent_lengths, inp_ent_mask, geo_data, query_edge, num_nodes, sentence_pointer, orig_inp, orig_inp_sent, bert_inp,\n",
    "                              inp_row_pos, bert_input_mask, bert_segment_ids]\n",
    "        return dataRows\n",
    "\n",
    "\n",
    "    def get_dataloader(self, mode='train', test_file='', bert_cache=None):\n",
    "        \"\"\"\n",
    "        Return a new SequenceDataLoader instance with appropriate rows\n",
    "        :param mode: train/val/test\n",
    "        :return: SequenceDataLoader object\n",
    "        \"\"\"\n",
    "        if mode != 'test':\n",
    "            if mode == 'train':\n",
    "                indices = self.train_indices\n",
    "            else:\n",
    "                indices = self.val_indices\n",
    "            dataRows = self._select(self.dataRows['train'], indices)\n",
    "        else:\n",
    "            dataRows = [v for k,v in self.dataRows['test'][test_file].items()]\n",
    "\n",
    "        logging.info(\"Total rows : {}, batches : {}\"\n",
    "                     .format(len(dataRows),\n",
    "                             len(dataRows) // self.batch_size))\n",
    "\n",
    "        collate_FN = collate_fn\n",
    "        if self.sentence_mode:\n",
    "            collate_FN = sent_collate_fn\n",
    "\n",
    "        dataRows = self.prepare_for_dataloader(dataRows, bert_cache)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return data.DataLoader(SequenceDataLoader(dataRows),\n",
    "                               batch_size=self.batch_size,\n",
    "                               num_workers=self.num_workers,\n",
    "                               collate_fn=collate_FN)\n",
    "                               \n",
    "        \"\"\"\n",
    "        batches = self.precompute_batches(dataRows)\n",
    "\n",
    "        return data.DataLoader(PreComputedDataLoader(batches),batch_size=1, collate_fn=pre_collate_fn)\n",
    "\n",
    "\n",
    "    def precompute_batches(self, dataRows:List[DataRow]):\n",
    "        print(\"precomputing batches...\")\n",
    "        batch_size = self.config.model.batch_size\n",
    "        batches = []\n",
    "        for i in range(0, len(dataRows), batch_size):\n",
    "            data = [dataRows[i].pattrs for i in range(i, i+batch_size) if i < len(dataRows)]\n",
    "            data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "            inp_data, s_inp_data, inp_ents, query, text_query, query_mask, target, text_target, \\\n",
    "            sent_lengths, inp_ent_mask, geo_data, query_edge, num_nodes, \\\n",
    "            sentence_pointer, orig_inp, orig_inp_sent, bert_inp, _, bert_input_mask, bert_segment_ids = zip(\n",
    "                *data)\n",
    "            inp_data, inp_lengths = simple_merge(inp_data)\n",
    "            s_inp_data, sent_lengths = sent_merge(s_inp_data, sent_lengths)\n",
    "            # outp_data, outp_lengths = simple_merge(outp_data)\n",
    "            text_target, text_target_lengths = simple_merge(text_target)\n",
    "            bert_input_mask, _ = simple_merge(bert_input_mask)\n",
    "            bert_segment_ids, _ = simple_merge(bert_segment_ids)\n",
    "            inp_ent_mask,_ = simple_merge(inp_ent_mask)\n",
    "\n",
    "            query = torch.LongTensor(query)\n",
    "            query_mask = pad_ents(query_mask, inp_lengths)\n",
    "            target = torch.LongTensor(target)\n",
    "            # geo_data_col, geo_data_slices = collate_geometric(geo_data)\n",
    "            slices = [p for n in num_nodes for p in n]\n",
    "            max_node = max(slices)\n",
    "            # add extra node to all graphs in order to have padding\n",
    "            geo_data = [GeometricData(x=torch.arange(max_node).unsqueeze(1), edge_index=gd['edge_index'],\n",
    "                                      edge_attr=gd['edge_attr'], y=gd['y']) for gd in geo_data]\n",
    "            geo_batch = GeometricBatch.from_data_list(geo_data)\n",
    "            # update the slices - same number of nodes\n",
    "            slices = [max_node for s in slices]\n",
    "            query_edge = torch.LongTensor(query_edge)\n",
    "            bert_inp,_ = simple_merge(bert_inp) #torch.cat(bert_inp, dim=0)\n",
    "            # assert bert_inp.size(0) == batch_size\n",
    "\n",
    "            # prepare batch\n",
    "            batch = Batch(\n",
    "                inp=inp_data,\n",
    "                s_inp=s_inp_data,\n",
    "                inp_lengths=inp_lengths,\n",
    "                sent_lengths=sent_lengths,\n",
    "                orig_inp=orig_inp,\n",
    "                orig_inp_sent=orig_inp_sent,\n",
    "                bert_inp=bert_inp,\n",
    "                target=target,\n",
    "                text_target=text_target,\n",
    "                text_target_lengths=text_target_lengths,\n",
    "                inp_ents=inp_ents,\n",
    "                query=query,\n",
    "                query_mask=query_mask,\n",
    "                inp_ent_mask=inp_ent_mask,\n",
    "                geo_batch=geo_batch,\n",
    "                query_edge=query_edge,\n",
    "                geo_slices=slices,\n",
    "                bert_segment_ids=bert_segment_ids,\n",
    "                bert_input_mask=bert_input_mask\n",
    "            )\n",
    "            #batch.to_device('cuda')\n",
    "            batches.append(batch)\n",
    "        print(\"done precomputing batches {}\".format(len(batches)))\n",
    "        return batches\n",
    "\n",
    "    def update_bert_cache(self, bert_cache:BertLocalCache):\n",
    "        \"\"\"\n",
    "        Preload all sentences from BERT\n",
    "        :param bert_cache:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logging.info(\"Bert caching train rows .. \")\n",
    "        for idx, dataRow in self.dataRows['train'].items():\n",
    "            bert_cache.update_cache(dataRow.story_sents)\n",
    "        logging.info(\"Bert caching test rows .. \")\n",
    "        for flname, dataRows in self.dataRows['test'].items():\n",
    "            for idx, dataRow in dataRows.items():\n",
    "                bert_cache.update_cache(dataRow.story_sents)\n",
    "        bert_cache.run_bert()\n",
    "\n",
    "\n",
    "    def map_text_to_id(self, text):\n",
    "        if isinstance(text, list):\n",
    "            return list(map(self.get_token, text))\n",
    "        else:\n",
    "            return self.get_token(text)\n",
    "\n",
    "    def get_token(self, word, target=False):\n",
    "        if target and word in self.target_word2id:\n",
    "            return self.target_word2id[word]\n",
    "        elif word in self.word2id:\n",
    "            return self.word2id[word]\n",
    "        else:\n",
    "            return self.word2id[UNK_WORD]\n",
    "\n",
    "    def get_entity_id(self, entity):\n",
    "        if entity in self.word2id:\n",
    "            return self.word2id[entity]\n",
    "        else:\n",
    "            return self.word2id[self.dummy_entity]\n",
    "\n",
    "    def _filter(self, array, mask):\n",
    "        \"\"\"\n",
    "        filter array based on boolean mask\n",
    "        :param array: any array\n",
    "        :param mask: boolean mask\n",
    "        :return: filtered\n",
    "        \"\"\"\n",
    "        return [array[i] for i,p in enumerate(mask) if p]\n",
    "\n",
    "    def _select(self, array, indices):\n",
    "        \"\"\"\n",
    "        Select based on ids\n",
    "        :param array:\n",
    "        :param indices:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return [array[i] for i in indices]\n",
    "\n",
    "    def __flatten__(self, arr):\n",
    "        if any(isinstance(el, list) for el in arr):\n",
    "            return [a for b in arr for a in b]\n",
    "        else:\n",
    "            return arr\n",
    "\n",
    "    def save(self, filename='data_files.pkl'):\n",
    "        \"\"\"\n",
    "        Save the current data utility into pickle file\n",
    "        :param filename: location\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        #pkl.dump(self.__dict__, open(filename, 'wb'))\n",
    "        logging.info(\"Saved data in {}\".format(filename))\n",
    "\n",
    "    def load(self, filename='data_files.pkl'):\n",
    "        \"\"\"\n",
    "        Load previously saved data utility\n",
    "        :param filename: location\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #logging.info(\"Loading data from {}\".format(filename))\n",
    "        #self.__dict__.update(pkl.load(open(filename,'rb')))\n",
    "        logging.info(\"Loaded\")\n",
    "\n",
    "\n",
    "class SequenceDataLoader(data.Dataset):\n",
    "    \"\"\"\n",
    "    Separate dataloader instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataRows:List[DataRow]):\n",
    "        \"\"\"\n",
    "        :param dataRows: training / validation / test data rows\n",
    "        :param data: pointer to DataUtility class\n",
    "        \"\"\"\n",
    "        self.dataRows = dataRows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return single training row for dataloader\n",
    "        :param item:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.dataRows[index].pattrs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataRows)\n",
    "\n",
    "\n",
    "class PreComputedDataLoader(data.Dataset):\n",
    "    \"\"\"\n",
    "    Separate dataloader instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batches):\n",
    "        \"\"\"\n",
    "        :param dataRows: training / validation / test data rows\n",
    "        :param data: pointer to DataUtility class\n",
    "        \"\"\"\n",
    "        self.batches = batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return single training row for dataloader\n",
    "        :param item:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.batches[index].clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "def pre_collate_fn(data):\n",
    "    assert len(data) == 1\n",
    "    return data[0]\n",
    "\n",
    "\n",
    "## Helper functions\n",
    "def simple_merge(rows):\n",
    "    lengths = [len(row) for row in rows]\n",
    "    padded_rows = pad_rows(rows, lengths)\n",
    "    return padded_rows, lengths\n",
    "\n",
    "def nested_merge(rows):\n",
    "    lengths = []\n",
    "    for row in rows:\n",
    "        row_length = [len(current_row) for current_row in row]\n",
    "        lengths.append(row_length)\n",
    "\n",
    "    # lengths = [len(row) for row in rows]\n",
    "    padded_rows = pad_nested_row(rows, lengths)\n",
    "    return padded_rows, lengths\n",
    "\n",
    "def simple_np_merge(rows):\n",
    "    lengths = [len(row) for row in rows]\n",
    "    padded_rows = pad_rows(rows, lengths)\n",
    "    return padded_rows, lengths\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    helper function for torch.DataLoader\n",
    "    :param data: list of tuples (inp, outp)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ## sort dataset by inp sentences\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    inp_data, s_inp_data, inp_ents, query, text_query, query_mask, target, text_target, sent_lengths, inp_ent_mask, geo_data, query_edge, num_nodes, *_ = zip(*data)\n",
    "    inp_data, inp_lengths = simple_merge(inp_data)\n",
    "    s_inp_data, sent_lengths = sent_merge(s_inp_data, sent_lengths)\n",
    "    # outp_data, outp_lengths = simple_merge(outp_data)\n",
    "    text_target, text_target_lengths = simple_merge(text_target)\n",
    "\n",
    "    query = torch.LongTensor(query)\n",
    "    query_mask = pad_ents(query_mask, inp_lengths)\n",
    "    target = torch.LongTensor(target)\n",
    "    #geo_data_col, geo_data_slices = collate_geometric(geo_data)\n",
    "    slices = [p for n in num_nodes for p in n]\n",
    "    max_node = max(slices)\n",
    "    # add extra node to all graphs in order to have padding\n",
    "    geo_data = [GeometricData(x=torch.arange(max_node).unsqueeze(1), edge_index=gd['edge_index'], edge_attr=gd['edge_attr'], y=gd['y']) for gd in geo_data]\n",
    "    geo_batch = GeometricBatch.from_data_list(geo_data)\n",
    "    # update the slices - same number of nodes\n",
    "    slices = [max_node for s in slices]\n",
    "    query_edge = torch.LongTensor(query_edge)\n",
    "\n",
    "    # prepare batch\n",
    "    batch = Batch(\n",
    "        inp=inp_data,\n",
    "        s_inp=s_inp_data,\n",
    "        inp_lengths=inp_lengths,\n",
    "        sent_lengths=sent_lengths,\n",
    "        target=target,\n",
    "        text_target=text_target,\n",
    "        text_target_lengths=text_target_lengths,\n",
    "        inp_ents=inp_ents,\n",
    "        query=query,\n",
    "        query_mask=query_mask,\n",
    "        inp_ent_mask = torch.LongTensor(inp_ent_mask),\n",
    "        geo_batch=geo_batch,\n",
    "        query_edge=query_edge,\n",
    "        geo_slices=slices\n",
    "    )\n",
    "\n",
    "    return batch\n",
    "\n",
    "def sent_merge(rows, sent_lengths):\n",
    "    \"\"\"\n",
    "    :param rows: [[[a,b],[c,d,e]], [[b,c,d,e],[d,e,f],[g,t]]]\n",
    "    :param sent_lengths: [[2,3],[4,3,2]]\n",
    "\n",
    "    padded_rows = 2 x 3 x 4\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lengths = [len(row) for row in rows] # number of sent in each batch\n",
    "    max_sent_l = max([n for sentl in sent_lengths for n in sentl]) # max number of words in each sent\n",
    "    padded_rows = torch.zeros(len(rows), max(lengths), max_sent_l).long()\n",
    "    for i,row in enumerate(rows):\n",
    "        end = lengths[i]\n",
    "        for j,sent_row in enumerate(row):\n",
    "            padded_rows[i, j, :sent_lengths[i][j]] = torch.LongTensor(sent_row)\n",
    "    # pad sent lengths\n",
    "    padded_lens = []\n",
    "    for srow in sent_lengths:\n",
    "        if len(srow) == max(lengths):\n",
    "            padded_lens.append(srow)\n",
    "        else:\n",
    "            srow.extend([0] * (max(lengths) - len(srow)))\n",
    "            padded_lens.append(srow)\n",
    "    return padded_rows, padded_lens\n",
    "\n",
    "def sent_collate_fn(data):\n",
    "    \"\"\"\n",
    "    helper function for torch.DataLoader\n",
    "    modified to handle sentences\n",
    "    :param data: list of tuples (inp, outp)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    ## sort dataset by number of sentences\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    inp_data, inp_ents, query, text_query, query_mask, target, text_target, inp_graphs\\\n",
    "        , sent_lengths, inp_ent_mask, sentence_pointer\\\n",
    "        , orig_inp, inp_row_pos = zip(*data)\n",
    "\n",
    "    inp_data, inp_lengths = sent_merge(inp_data, sent_lengths)\n",
    "    inp_row_pos, _ = sent_merge(inp_row_pos, sent_lengths)\n",
    "    max_node, _, _ = sentence_pointer[0].shape\n",
    "    sentence_pointer = [sp.reshape(-1, sp.shape[2]) for sp in sentence_pointer]\n",
    "    sentence_pointer = [sp.tolist() for sp in sentence_pointer]\n",
    "    sentence_pointer = [s for sp in sentence_pointer for s in sp] # flatten\n",
    "    sentence_pointer, sent_lens = simple_merge(sentence_pointer)\n",
    "    sentence_pointer = sentence_pointer.view(inp_data.size(0), max_node, max_node, -1)\n",
    "\n",
    "    sent_lengths = pad_sent_lengths(sent_lengths)\n",
    "\n",
    "    text_target, text_target_lengths = simple_merge(text_target)\n",
    "    query = torch.LongTensor(query)\n",
    "    query_mask = pad_ents(query_mask, inp_lengths)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    # prepare batch\n",
    "    batch = Batch(\n",
    "        inp=inp_data,\n",
    "        inp_lengths=inp_lengths,\n",
    "        sent_lengths=sent_lengths,\n",
    "        target=target,\n",
    "        text_target=text_target,\n",
    "        text_target_lengths=text_target_lengths,\n",
    "        inp_ents=inp_ents,\n",
    "        query=query,\n",
    "        query_mask=query_mask,\n",
    "        inp_graphs=torch.LongTensor(inp_graphs),\n",
    "        sentence_pointer=sentence_pointer,\n",
    "        orig_inp = orig_inp,\n",
    "        inp_ent_mask = torch.LongTensor(inp_ent_mask),\n",
    "        inp_row_pos = inp_row_pos\n",
    "    )\n",
    "\n",
    "\n",
    "    return batch\n",
    "\n",
    "def pad_rows(rows, lengths):\n",
    "    padded_rows = torch.zeros(len(rows), max(lengths)).long()\n",
    "    for i, row in enumerate(rows):\n",
    "        end = lengths[i]\n",
    "        padded_rows[i, :end] = torch.LongTensor(row[:end])\n",
    "    return padded_rows\n",
    "\n",
    "def pad_nested_row(rows, lengths):\n",
    "    max_abstract_length = max([l for ln in lengths for l in ln])\n",
    "    max_num_abstracts = max(list(map(len, rows)))\n",
    "    padded_rows = torch.zeros(len(rows), max_num_abstracts, max_abstract_length).long()\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, abstract in enumerate(row):\n",
    "            end = lengths[i][j]\n",
    "            padded_rows[i, j, :end] = torch.LongTensor(row[j][:end])\n",
    "    return padded_rows\n",
    "\n",
    "\n",
    "def pad_ents(ents, lengths):\n",
    "    padded_ents = torch.zeros((len(ents), max(lengths), 2)).long()\n",
    "    for i, row in enumerate(ents):\n",
    "        end = lengths[i]\n",
    "        for ent_n in range(len(row)):\n",
    "            padded_ents[i, :end, ent_n] = torch.LongTensor(row[ent_n][:end])\n",
    "    return padded_ents\n",
    "\n",
    "def pad_nested_ents(ents, lengths):\n",
    "    abstract_lengths = []\n",
    "    batch_size = len(ents)\n",
    "    abstracts_per_batch = len(ents[0])\n",
    "    num_entities = len(ents[0][0])\n",
    "    abstract_lengths = []\n",
    "    for row in ents:\n",
    "        row_length = [len(abstract_line[0]) for abstract_line in row]\n",
    "        abstract_lengths.append(row_length)\n",
    "    abstract_lengths = [a for c in abstract_lengths for a in c]\n",
    "    max_abstract_length = max(abstract_lengths)\n",
    "    padded_ents = torch.zeros(batch_size, abstracts_per_batch, num_entities, max_abstract_length).long()\n",
    "    for i, batch_row in enumerate(ents):\n",
    "        for j, abstract in enumerate(batch_row):\n",
    "            for ent_n in range(len(abstract)):\n",
    "                end = lengths[i]\n",
    "                padded_ents[i, j, ent_n, :end] = torch.LongTensor(batch_row[j][ent_n][:end])\n",
    "    return padded_ents\n",
    "\n",
    "def pad_sent_lengths(sent_lens):\n",
    "    \"\"\"\n",
    "    given sentence lengths, pad them so that the total batch length is equal\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_len = max([len(sent) for sent in sent_lens])\n",
    "    pad_lens = []\n",
    "    for sent in sent_lens:\n",
    "        pad_lens.append(sent + [0]*(max_len - len(sent)))\n",
    "    return pad_lens\n",
    "\n",
    "def collate_geometric(data_list):\n",
    "    r\"\"\"Collates a python list of data objects to the internal storage\n",
    "    format of :class:`torch_geometric.data.InMemoryDataset`.\"\"\"\n",
    "    keys = data_list[0].keys\n",
    "    data = GeometricData()\n",
    "\n",
    "    for key in keys:\n",
    "        data[key] = []\n",
    "    slices = {key: [0] for key in keys}\n",
    "\n",
    "    for item, key in product(data_list, keys):\n",
    "        data[key].append(item[key])\n",
    "        s = slices[key][-1] + item[key].size(item.cat_dim(key, item[key]))\n",
    "        slices[key].append(s)\n",
    "\n",
    "    for key in keys:\n",
    "        data[key] = torch.cat(\n",
    "            data[key], dim=data_list[0].cat_dim(key, data_list[0][key]))\n",
    "        slices[key] = torch.LongTensor(slices[key])\n",
    "\n",
    "    return data, slices\n",
    "\n",
    "def generate_dictionary(config):\n",
    "    \"\"\"\n",
    "    Before running an experiment, make sure that a dictionary\n",
    "    is generated\n",
    "    Check if the dictionary is present, if so then return\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.abspath(os.pardir).split('/codes')[0]\n",
    "    dictionary_file = os.path.join(parent_dir, 'data', config.dataset.data_path, 'dict.json')\n",
    "    if os.path.isfile(dictionary_file):\n",
    "        logging.info(\"Dictionary present at {}\".format(dictionary_file))\n",
    "        return\n",
    "    logging.info(\"Creating dictionary with all test files\")\n",
    "    ds = DataUtility(config)\n",
    "    datas = []\n",
    "    logging.info(\"For training file\")\n",
    "    train_data, max_ents = ds.process_data(os.path.join(parent_dir, 'data', config.dataset.data_path),\n",
    "                    config.dataset.train_file, load_dictionary=False, preprocess=False)\n",
    "    datas.append(train_data)\n",
    "    logging.info(\"For testing files\")\n",
    "    for test_file in config.dataset.test_files:\n",
    "        logging.info(\"For file {}\".format(test_file))\n",
    "        test_data, max_e = ds.process_data(os.path.join(parent_dir, 'data', config.dataset.data_path),\n",
    "                        test_file, load_dictionary=False, preprocess=False)\n",
    "        datas.append(test_data)\n",
    "        if max_e > max_ents:\n",
    "            max_ents = max_e\n",
    "    ds.max_ents = max_ents\n",
    "    logging.info(\"Processing words...\")\n",
    "    for data in datas:\n",
    "        ds.preprocess(data)\n",
    "\n",
    "    # save dictionary\n",
    "    dictionary = {\n",
    "        'word2id': ds.word2id,\n",
    "        'id2word': ds.id2word,\n",
    "        'target_word2id': ds.target_word2id,\n",
    "        'target_id2word': ds.target_id2word,\n",
    "        'max_ents': ds.max_ents,\n",
    "        'max_vocab': ds.max_vocab,\n",
    "        'max_entity_id': ds.max_entity_id,\n",
    "        'entity_ids': ds.entity_ids,\n",
    "        'dummy_entitiy': ds.dummy_entity,\n",
    "        'entity_map': ds.entity_map\n",
    "    }\n",
    "    json.dump(dictionary, open(dictionary_file,'w'))\n",
    "    logging.info(\"Saved dictionary at {}\".format(dictionary_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import string\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> List[str]:\n",
    "    \"\"\"Normalize string and split string into tokens.\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match score.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1_from_tokens(gold_toks: List[str], pred_toks: List[str]):\n",
    "    \"\"\"Compute the F1 score from tokenized gold answer and prediction.\"\"\"\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    #print(precision)\n",
    "    #print(recall)\n",
    "  \n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str):\n",
    "    \"\"\"Compute the F1 score.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    return compute_f1_from_tokens(gold_toks, pred_toks)\n",
    "\n",
    "import statistics\n",
    "def f1_eval(formatted_predictions, references):\n",
    "    f1_scores = []\n",
    "    \n",
    "    for i,pair in enumerate(formatted_predictions):\n",
    "        #print(compute_f1(formatted_predictions[i],references[i]))\n",
    "        f1 = compute_f1(formatted_predictions[i],references[i])\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    #print(f1_scores)\n",
    "    print('F1 score is: '+str(statistics.mean(f1_scores)))\n",
    "    return statistics.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_util = DataUtility(config)\n",
    "    data_base_path = os.path.join(parent_dir, 'data', config.dataset.data_path)\n",
    "    data_pkl_path = os.path.join(data_base_path, config.dataset.save_path)\n",
    "    if config.dataset.load_save_path or config.general.mode == 'infer' or resume:\n",
    "        data_util.load(data_pkl_path)\n",
    "    else:\n",
    "        data_util.process_data(base_path,\n",
    "                               config.dataset.train_file, load_dictionary=True)\n",
    "        data_util.save(data_pkl_path)\n",
    "\n",
    "    vocab_size = len(data_util.word2id)\n",
    "    config.log.logger.info(\"Vocab Size : {}\".format(vocab_size))\n",
    "    target_size = len(data_util.target_word2id)\n",
    "    config.log.logger.info(\"Target size : {}\".format(target_size))\n",
    "    config.model.vocab_size = vocab_size\n",
    "    config.model.target_size = target_size\n",
    "    config.model.max_nodes = data_util.num_entity_block\n",
    "    config.model.max_sent_length = data_util.max_sent_length\n",
    "    config.model.classes = data_util.target_id2word\n",
    "\n",
    "    config.log.logger.info(\"Loading testing data\")\n",
    "    data_util.process_test_data(base_path, config.dataset.test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForClassification.from_pretrained(\"bert-base-cased\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-clutrr\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset=tokenized_validation_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epoch= 0\n",
    "f1 = 0\n",
    "best_model_id = 0\n",
    "for i in range(epoch):\n",
    "    if i != 0:\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(\"./test-clutrr-5\")\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_test_datasets,\n",
    "        eval_dataset=tokenized_validation_datasets,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "    raw_predictions = trainer.predict(eval_features)\n",
    "    validation_predictions = postprocess_qa_predictions(validation_ds, eval_features, raw_predictions.predictions)\n",
    "    formatted_predictions = [v[0] for k, v in validation_predictions.items()]\n",
    "    references = [ex[\"target\"] for ex in validation_ds]\n",
    "    new_f1 = f1_eval(formatted_predictions, references)\n",
    "    trainer.save_model(\"test-clutrr-5\")\n",
    "    if new_f1 > f1:\n",
    "        f1 = new_f1\n",
    "        trainer.save_model(\"test-clutrr-5-best\")\n",
    "        best_model_id = i\n",
    "        print(\"The best trained model so far is trained at epoch NO.\"+str(best_model_id))\n",
    "\n",
    "# In[ ]:\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./test-clutrr-5-best\")\n",
    "trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=tokenized_test_datasets,\n",
    "        eval_dataset=tokenized_validation_datasets,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    ")\n",
    "print(\"The best trained model is trained at epoch NO.\"+str(best_model_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
